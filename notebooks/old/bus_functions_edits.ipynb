{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"/home/canyon/Bus-Weather-Impacts\")\n",
    "from src.utils import *\n",
    "import pandas as pd\n",
    "import os\n",
    "import osmnx as ox\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "from sklearn.neighbors import KDTree\n",
    "import numpy as np\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.float_format', '{:.02f}'.format)\n",
    "from geopy.distance import geodesic\n",
    "from shapely.geometry import Point\n",
    "calculated_pair_path = \"data/node_pairs.parquet\"\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import networkx as nx\n",
    "from typing import Dict, List, Tuple, Union\n",
    "from functools import lru_cache\n",
    "\n",
    "# Assuming these functions are imported from elsewhere in the codebase\n",
    "# from utils import read_parquet_from_tar_gz, ox, KDTree\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def compute_distance(prev_osmid: int, osmid: int, graph: nx.Graph) -> Union[float, None]:\n",
    "    \"\"\"Compute the shortest path length between two nodes.\"\"\"\n",
    "    try:\n",
    "        return nx.shortest_path_length(graph, prev_osmid, osmid, weight='travel_time')\n",
    "    except nx.NetworkXNoPath:\n",
    "        return None\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def compute_path(prev_osmid: int, osmid: int, graph: nx.Graph) -> Union[List[int], None]:\n",
    "    \"\"\"Compute the shortest path between two nodes.\"\"\"\n",
    "    try:\n",
    "        return nx.shortest_path(graph, prev_osmid, osmid, weight='travel_time')\n",
    "    except nx.NetworkXNoPath:\n",
    "        return None\n",
    "\n",
    "def compute_euclid_dists(node_pairs: pd.DataFrame, nodes_points: gpd.GeoDataFrame) -> pd.Series:\n",
    "    \"\"\"Compute Euclidean distances between node pairs.\"\"\"\n",
    "    nodes_points = nodes_points.to_crs(2263)\n",
    "    nodes_points_xy = nodes_points[['osmid', 'geometry']].copy()\n",
    "    nodes_points_xy['x'] = nodes_points_xy['geometry'].x\n",
    "    nodes_points_xy['y'] = nodes_points_xy['geometry'].y\n",
    "\n",
    "    merged = node_pairs.merge(nodes_points_xy, left_on=\"osmid\", right_on=\"osmid\", how=\"left\")\n",
    "    merged = merged.merge(nodes_points_xy, right_on=\"osmid\", left_on=\"prev_osmid\", how=\"left\", suffixes=[\"_curr\", \"_prev\"])\n",
    "    \n",
    "    x_diff_sq = (merged['x_curr'] - merged['x_prev'])**2\n",
    "    y_diff_sq = (merged['y_curr'] - merged['y_prev'])**2\n",
    "\n",
    "    return np.sqrt(x_diff_sq + y_diff_sq) / 3.28\n",
    "\n",
    "def precalculate_node_pair_distances(node_pair_df: pd.DataFrame, calculated_pair_path: str, G: nx.Graph, nodes: gpd.GeoDataFrame) -> None:\n",
    "    \"\"\"Precalculate and save node pair distances.\"\"\"\n",
    "    try:\n",
    "        calculated_pairs = pd.read_parquet(calculated_pair_path)\n",
    "    except Exception:\n",
    "        print(\"No pre-calculated pairs found\")\n",
    "        calculated_pairs = pd.DataFrame(columns=[\"osmid\", \"prev_osmid\", \"distance_osm\", \"distance_euclid\", \"shortest_path\", \"dist_ratio\"])\n",
    "\n",
    "    node_pair_df = node_pair_df.drop_duplicates().dropna()\n",
    "    node_pair_df = node_pair_df.merge(calculated_pairs, on=[\"osmid\", \"prev_osmid\"], how=\"outer\")\n",
    "    pairs_to_calc = node_pair_df[node_pair_df[\"distance_euclid\"].isna()].reset_index(drop=True)\n",
    "    print(f\"Pairs to calculate: {pairs_to_calc.shape[0]}\")\n",
    "    \n",
    "    if not pairs_to_calc.empty:\n",
    "        pairs_to_calc[\"distance_osm\"] = pairs_to_calc.apply(lambda row: compute_distance(row['prev_osmid'], row['osmid'], G), axis=1)\n",
    "        pairs_to_calc[\"distance_euclid\"] = compute_euclid_dists(pairs_to_calc, nodes)\n",
    "        pairs_to_calc[\"shortest_path\"] = pairs_to_calc.apply(lambda row: compute_path(row['prev_osmid'], row['osmid'], G), axis=1)\n",
    "        pairs_to_calc[\"dist_ratio\"] = pairs_to_calc[\"distance_euclid\"] / pairs_to_calc[\"distance_osm\"]\n",
    "\n",
    "        calculated_pairs = pd.concat([calculated_pairs, pairs_to_calc])\n",
    "        calculated_pairs.to_parquet(calculated_pair_path)\n",
    "        print(f\"Wrote calculated pairs to {calculated_pair_path}\")\n",
    "    else:\n",
    "        print(\"No new pairs to calculate\")\n",
    "\n",
    "def prep_buses_nodes(buses_with_nodes: gpd.GeoDataFrame, max_distance_to_node: float) -> pd.DataFrame:\n",
    "    \"\"\"Prepare bus node data.\"\"\"\n",
    "    buses_with_nodes = (\n",
    "        buses_with_nodes.sort_values([\"trip_id\", \"timestamp\"])\n",
    "        .drop_duplicates(subset=[\"trip_id\", \"osmid\"], keep=\"first\")\n",
    "        .to_crs(2263)\n",
    "    )\n",
    "    \n",
    "    columns = [\"route_short\", \"timestamp\", \"trip_id\", \"next_stop_id\", \"osmid\", \"vehicle_id\", \"distance_to_node\", \"geometry\"]\n",
    "    buses_with_nodes = buses_with_nodes[columns].copy()\n",
    "    \n",
    "    buses_with_nodes[\"prev_stop_id\"] = buses_with_nodes.groupby(\"trip_id\")[\"next_stop_id\"].shift(1)\n",
    "    buses_with_nodes[\"prev_osmid\"] = buses_with_nodes.groupby(\"trip_id\")[\"osmid\"].shift(1)\n",
    "    buses_with_nodes[\"next_osmid\"] = buses_with_nodes.groupby(\"trip_id\")[\"osmid\"].shift(-1)\n",
    "\n",
    "    buses_with_nodes[\"prev_osmid\"] = buses_with_nodes[\"prev_osmid\"].astype(float)\n",
    "    buses_with_nodes[\"osmid\"] = buses_with_nodes[\"osmid\"].astype(float)\n",
    "\n",
    "    return buses_with_nodes[buses_with_nodes[\"distance_to_node\"] < max_distance_to_node]\n",
    "\n",
    "def tag_feed_with_nodes(buses: pd.DataFrame, tree: KDTree, nodes: gpd.GeoDataFrame, types_to_include: List[Union[str, float]] = [np.NaN, \"traffic_signals\", \"stop\"]) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Tag bus feed with nearest nodes.\"\"\"\n",
    "    nodes = nodes[nodes[\"highway\"].isin(types_to_include)]\n",
    "    nearest_nodes = tree.query(np.array(buses[['lat', 'lon']]), k=1, return_distance=False)\n",
    "    buses['nearest_node'] = nearest_nodes.flatten()\n",
    "\n",
    "    buses['nearest_osm_id'] = buses['nearest_node'].map(nodes['osmid'])\n",
    "    buses = buses.merge(nodes, left_on=\"nearest_osm_id\", right_on=\"osmid\")\n",
    "    return gpd.GeoDataFrame(buses, geometry='geometry')\n",
    "\n",
    "def get_node_data(place: str = \"New York City, New York, USA\") -> Tuple[KDTree, gpd.GeoDataFrame, nx.Graph]:\n",
    "    \"\"\"Get node data for a specified place.\"\"\"\n",
    "    G = ox.graph_from_place(place, network_type='drive')\n",
    "    G = ox.add_edge_speeds(G)\n",
    "    G = ox.add_edge_travel_times(G)\n",
    "    \n",
    "    nodes = ox.graph_to_gdfs(G, edges=False).reset_index()\n",
    "    tree = KDTree(nodes[['y', 'x']], metric='euclidean')\n",
    "\n",
    "    return tree, nodes, G\n",
    "\n",
    "def prep_coords(df: pd.DataFrame, lat_col: str, lon_col: str) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Prepare coordinates for geospatial analysis.\"\"\"\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df,\n",
    "        geometry=gpd.points_from_xy(df[lon_col], df[lat_col]),\n",
    "        crs=4326\n",
    "    )\n",
    "\n",
    "    gdf_projected = gdf.to_crs(2263)\n",
    "    gdf[\"planar_x\"] = gdf_projected.geometry.x\n",
    "    gdf[\"planar_y\"] = gdf_projected.geometry.y\n",
    "    \n",
    "    return gdf.drop('geometry', axis=1)\n",
    "\n",
    "def calculate_distance_to_node(buses_with_nodes: gpd.GeoDataFrame) -> pd.Series:\n",
    "    \"\"\"Calculate distance from buses to their nearest nodes.\"\"\"\n",
    "    buses_projected = buses_with_nodes.to_crs(2263)\n",
    "    return np.sqrt((buses_projected.geometry.x - buses_with_nodes[\"planar_x\"])**2 + \n",
    "                   (buses_projected.geometry.y - buses_with_nodes[\"planar_y\"])**2)\n",
    "\n",
    "def calculate_speeds(prepped_trips: pd.DataFrame, calculated_pair_path: str = \"data/node_pairs.parquet\", minimum_time_diff: int = 45) -> pd.DataFrame:\n",
    "    \"\"\"Calculate bus speeds based on node pair distances.\"\"\"\n",
    "    node_pair_dists = pd.read_parquet(calculated_pair_path)\n",
    "\n",
    "    prepped_trips[\"time_diff_seconds\"] = prepped_trips.groupby(\"trip_id\")[\"timestamp\"].diff().dt.total_seconds()\n",
    "    prepped_trips = prepped_trips[prepped_trips[\"time_diff_seconds\"] >= minimum_time_diff]\n",
    "\n",
    "    buses_with_distances = prepped_trips.merge(node_pair_dists)\n",
    "    buses_with_distances[\"speed_osm\"] = (buses_with_distances[\"distance_osm\"] / 1609) / (buses_with_distances[\"time_diff_seconds\"] / 3600)\n",
    "    buses_with_distances[\"speed_euclid\"] = (buses_with_distances[\"distance_euclid\"] / 1609) / (buses_with_distances[\"time_diff_seconds\"] / 3600)\n",
    "    \n",
    "    return buses_with_distances\n",
    "\n",
    "def explode_edges(row: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"Explode edges from a single row into multiple rows.\"\"\"\n",
    "    try:\n",
    "        nodes = row['shortest_path']\n",
    "        return pd.DataFrame({\n",
    "            'idx': [int(row['index'])] * (len(nodes) - 1),\n",
    "            'from': nodes[:-1],\n",
    "            'to': nodes[1:]\n",
    "        })\n",
    "    except:\n",
    "        return pd.DataFrame({'idx': [pd.NA], 'from': [pd.NA], 'to': [pd.NA]})\n",
    "\n",
    "def get_bus_stops(path: str = \"/home/data/test/cities/C3562/stops.geojson\") -> gpd.GeoDataFrame:\n",
    "    \"\"\"Get bus stop data from a GeoJSON file.\"\"\"\n",
    "    bus_stops = gpd.read_file(path)\n",
    "    bus_agencies = [\"MTA NYCT\", \"MTABC\", \"MTA NYCT,MTABC\"]\n",
    "    bus_stops = bus_stops[bus_stops[\"agency_ids_serviced\"].isin(bus_agencies)][[\"stop_id\", \"stop_name\", \"stop_lat\", \"stop_lon\", \"geometry\"]]\n",
    "    bus_stops = bus_stops.rename({\"stop_lat\": \"lat\", \"stop_lon\": \"lon\"}, axis=1)\n",
    "    bus_stops[\"stop_id\"] = \"MTA_\" + bus_stops[\"stop_id\"]\n",
    "    return prep_coords(bus_stops, \"lat\", \"lon\")\n",
    "\n",
    "def bus_stops_nodes(bus_stops: gpd.GeoDataFrame, tree: KDTree, nodes: gpd.GeoDataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Tag bus stops with nearest nodes.\"\"\"\n",
    "    stops_with_nodes = tag_feed_with_nodes(bus_stops, tree, nodes)\n",
    "    stops_with_nodes[\"dist_to_node\"] = calculate_distance_to_node(stops_with_nodes)\n",
    "    stops_with_nodes = stops_with_nodes[stops_with_nodes[\"dist_to_node\"] < 200]\n",
    "    return stops_with_nodes[[\"stop_id\", \"stop_name\", \"osmid\", \"dist_to_node\"]]\n",
    "\n",
    "def get_stop_pairs(bus_stops: pd.DataFrame, raw_GTFS_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Get pairs of consecutive bus stops from GTFS data.\"\"\"\n",
    "    if raw_GTFS_path.endswith('.gz'):\n",
    "        gtfs_rt = read_parquet_from_tar_gz(raw_GTFS_path)\n",
    "    else:\n",
    "        col_remappings = {\n",
    "            \"vehicle.trip.trip_id\": \"trip_id\",\n",
    "            \"vehicle.timestamp\": \"timestamp\",\n",
    "            \"vehicle.position.latitude\": \"lat\",\n",
    "            \"vehicle.position.longitude\": \"lon\",\n",
    "            \"vehicle.trip.route_id\": \"route_short\",\n",
    "            \"vehicle.stop_id\": \"next_stop_id\",\n",
    "            \"vehicle.vehicle.id\": \"vehicle_id\"\n",
    "        }\n",
    "        gtfs_rt = pd.read_parquet(raw_GTFS_path).rename(columns=col_remappings)\n",
    "        gtfs_rt[\"next_stop_id\"] = \"MTA_\" + gtfs_rt[\"next_stop_id\"]\n",
    "\n",
    "    \n",
    "    gtfs_rt = gtfs_rt.merge(bus_stops, left_on=\"next_stop_id\", right_on=\"stop_id\", how=\"left\")\n",
    "    gtfs_rt = gtfs_rt[[\"trip_id\", \"route_short\", \"timestamp\", \"next_stop_id\"]].sort_values([\"trip_id\", \"timestamp\"]).drop_duplicates([\"trip_id\", \"next_stop_id\"]).dropna()\n",
    "    gtfs_rt[\"prev_stop_id\"] = gtfs_rt.groupby(\"trip_id\")[\"next_stop_id\"].shift(1)\n",
    "\n",
    "    stop_pairs = gtfs_rt[[\"prev_stop_id\", \"next_stop_id\"]]\n",
    "    stop_pairs = stop_pairs.merge(bus_stops[[\"stop_id\", \"stop_name\", \"osmid\"]], left_on=\"prev_stop_id\", right_on=\"stop_id\")\n",
    "    stop_pairs = stop_pairs.merge(bus_stops[[\"stop_id\", \"stop_name\", \"osmid\"]], left_on=\"next_stop_id\", right_on=\"stop_id\", suffixes=[\"_prev\", \"_next\"])\n",
    "    stop_pairs = stop_pairs.rename(columns={\"osmid_next\": \"osmid\", \"osmid_prev\": \"prev_osmid\"})\n",
    "    stop_pairs[\"osmid\"] = stop_pairs[\"osmid\"].astype(int)\n",
    "    stop_pairs[\"prev_osmid\"] = stop_pairs[\"prev_osmid\"].astype(int)\n",
    "\n",
    "    return stop_pairs.drop_duplicates()\n",
    "\n",
    "def get_pair_paths(stop_pairs: pd.DataFrame, G: nx.Graph, nodes: gpd.GeoDataFrame, calculated_pair_path: str = \"data/node_pairs.parquet\") -> pd.DataFrame:\n",
    "    \"\"\"Get paths between pairs of stops.\"\"\"\n",
    "    precalculate_node_pair_distances(stop_pairs[[\"osmid\", \"prev_osmid\"]], calculated_pair_path, G, nodes)\n",
    "    node_pair_dists = pd.read_parquet(calculated_pair_path)\n",
    "    return stop_pairs[[\"osmid\", \"prev_osmid\", \"next_stop_id\", \"prev_stop_id\", \"stop_name_prev\", \"stop_name_next\"]].merge(node_pair_dists)\n",
    "\n",
    "def full_process_stops(tree: KDTree, nodes: gpd.GeoDataFrame, G: nx.Graph, GTFS_PATH: str, calculated_pair_path: str = \"data/node_pairs.parquet\", stops_path: str = \"/home/data/test/cities/C3562/stops.geojson\") -> pd.DataFrame:\n",
    "    \"\"\"Fully process bus stops data.\"\"\"\n",
    "    bus_stops = get_bus_stops(stops_path)\n",
    "    bus_stops = bus_stops_nodes(bus_stops, tree, nodes)\n",
    "    stop_pairs = get_stop_pairs(bus_stops, GTFS_PATH)\n",
    "    stop_pairs = get_pair_paths(stop_pairs, G, nodes, calculated_pair_path)\n",
    "\n",
    "    return stop_pairs[[\"next_stop_id\", \"prev_stop_id\", \"stop_name_prev\", \"stop_name_next\", \"shortest_path\"]].rename(columns={\"shortest_path\": \"shortest_path_stops\"})\n",
    "\n",
    "def check_in_bus_path(row: pd.Series) -> bool:\n",
    "    \"\"\"Check if a node is in the bus path.\"\"\"\n",
    "    return isinstance(row[\"shortest_path_stops\"], (list, np.ndarray)) and row[\"osmid\"] in row[\"shortest_path_stops\"]\n",
    "\n",
    "def process_gtfs_rt_main(tree: KDTree, nodes: gpd.GeoDataFrame, G: nx.Graph, gtfs_path: str, calculated_pair_path: str, out_path: str, stops_with_paths: pd.DataFrame = None, max_distance_to_node: float = 100) -> None:\n",
    "    \"\"\"Main function to process GTFS realtime data.\"\"\"\n",
    "    print(\"Preprocessing bus data\")\n",
    "    if gtfs_path.endswith('.gz'):\n",
    "        buses = read_parquet_from_tar_gz(gtfs_path)\n",
    "    else:\n",
    "        col_remappings = {\n",
    "            \"vehicle.trip.trip_id\": \"trip_id\",\n",
    "            \"vehicle.timestamp\": \"timestamp\",\n",
    "            \"vehicle.position.latitude\": \"lat\",\n",
    "            \"vehicle.position.longitude\": \"lon\",\n",
    "            \"vehicle.trip.route_id\": \"route_short\",\n",
    "            \"vehicle.stop_id\": \"next_stop_id\",\n",
    "            \"vehicle.vehicle.id\": \"vehicle_id\"\n",
    "        }\n",
    "        buses = pd.read_parquet(gtfs_path).rename(columns=col_remappings)\n",
    "        buses[\"next_stop_id\"] = \"MTA_\" + buses[\"next_stop_id\"]\n",
    "    buses = prep_coords(buses, 'lat', 'lon')\n",
    "\n",
    "    print(\"Tagging bus locations with nodes\")\n",
    "    buses_with_nodes = tag_feed_with_nodes(buses, tree, nodes)\n",
    "    buses_with_nodes[\"distance_to_node\"] = calculate_distance_to_node(buses_with_nodes)\n",
    "\n",
    "    print(\"Calculating distance pairs\")\n",
    "    prepped_trips = prep_buses_nodes(buses_with_nodes, max_distance_to_node)\n",
    "\n",
    "    if stops_with_paths is not None:\n",
    "        print(f\"Initial shape: {prepped_trips.shape}\")\n",
    "        prepped_trips = prepped_trips.merge(stops_with_paths)\n",
    "        prepped_trips[\"in_bus_path\"] = prepped_trips.apply(check_in_bus_path, axis=1)\n",
    "        prepped_trips = prepped_trips[prepped_trips[\"in_bus_path\"]]\n",
    "        print(f\"Shape after filtering: {prepped_trips.shape}\")\n",
    "\n",
    "    node_pair_df = prepped_trips[[\"osmid\", \"prev_osmid\"]]\n",
    "\n",
    "    precalculate_node_pair_distances(node_pair_df, calculated_pair_path, G, nodes)\n",
    "    buses_with_speeds = calculate_speeds(prepped_trips).reset_index()\n",
    "\n",
    "    print(\"Exploding edges\")\n",
    "    segment_speeds = pd.concat([explode_edges(row) for _, row in buses_with_speeds.iterrows()])\n",
    "    bus_speed_segmented = buses_with_speeds.merge(segment_speeds, left_on=\"index\", right_on=\"idx\").drop(columns=[\"index\"])\n",
    "\n",
    "    print(\"Writing to parquet\")\n",
    "    bus_speed_segmented.to_parquet(out_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_parquet_from_tar_gz(\"https://urbantech-public.s3.amazonaws.com/DO-NOT-DELETE-BUSOBSERVATORY-PUBLIC-DATASET/one-system-day.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GTFS_PATH = \"https://urbantech-public.s3.amazonaws.com/DO-NOT-DELETE-BUSOBSERVATORY-PUBLIC-DATASET/one-system-day.tar.gz\"\n",
    "GTFS_PATH = \"/home/data/bus-weather/raw_bus_gtfs_rt_202230917_20230930.parquet\"\n",
    "CALCULATED_PAIR_PATH = \"data/node_pairs.parquet\"\n",
    "OUT_PATH = \"data/buses_with_segmented_storm.parquet\"\n",
    "#OUT_PATH = \"data/buses_test.parquet\"\n",
    "STOPS_PATH = \"/home/data/test/cities/C3562/stops.geojson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_parquet_from_tar_gz(\"https://urbantech-public.s3.amazonaws.com/DO-NOT-DELETE-BUSOBSERVATORY-PUBLIC-DATASET/one-system-day.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(GTFS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree, nodes, G = get_node_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_with_paths = full_process_stops(tree, nodes, G, GTFS_PATH, calculated_pair_path = CALCULATED_PAIR_PATH, stops_path = \"/home/data/test/cities/C3562/stops.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_with_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_gtfs_rt_main(tree, nodes, G, GTFS_PATH, CALCULATED_PAIR_PATH, OUT_PATH, stops_with_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd.read_parquet(OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_buses = gpd.read_parquet(OUT_PATH)\n",
    "old_buses = gpd.read_parquet(\"/home/canyon/Bus-Weather-Impacts/data/buses_with_segmented.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.01, 0.1, 0.25, 0.5, 0.75, 0.9, .99]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_buses[[\"speed_osm\", \"speed_euclid\", \"dist_ratio\"]].describe(percentiles=quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_buses[[\"speed_osm\", \"speed_euclid\", \"dist_ratio\"]].describe(percentiles=quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_speed_segemented.query(\"speed_osm < 70\").query(\"`from` == 4209661118 & to == 4209661121.00\")['speed_osm'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_speeds.to_parquet(\"segments_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepped_trips[\"time_diff_seconds\"] = prepped_trips.groupby(\"trip_id\")[\"timestamp\"].diff().dt.total_seconds()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TOP-Sprint-lAvM2-mU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
